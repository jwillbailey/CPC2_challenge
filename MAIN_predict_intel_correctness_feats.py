import wandb
import argparse
import csv
import datetime
import json
import os
# from process_cpc2_data import get_cpc2_dataset
# from transformers import WhisperProcessor
import random

import numpy
import numpy as np
import pandas as pd
import speechbrain as sb
import torch
import torch.nn as nn
import torch.optim as optim
import torchaudio.transforms as T

from attrdict import AttrDict
from scipy.optimize import curve_fit
from scipy.stats import spearmanr
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
from tqdm import tqdm

from constants import DATAROOT, DATAROOT_CPC1
from data_handling import get_disjoint_val_set
from exemplar import get_ex_set
from models.ni_feat_extractors import (HuBERTEncoder_feats, HuBERTFull_feats,
                                       Spec_feats, WhisperBase_feats,
                                       WhisperEncoder_feats, WhisperFull_feats,
                                       XLSREncoder_feats, XLSRFull_feats)
from models.ni_predictor_models import (ExLSTM, ExLSTM_div, ExLSTM_layers,
                                        ExLSTM_log, ExLSTM_std,
                                        MetricPredictorAttenPool,
                                        MetricPredictorLSTM,
                                        MetricPredictorLSTM_layers, wordLSTM)

#from GHA import audiogram



def rmse_score(x, y):
    return np.sqrt(np.mean((x - y) ** 2))


def std_err(x, y):
    return np.std(x - y) / np.sqrt(len(x))


def compute_feats(wavs,resampler):
    """Feature computation pipeline"""
    wavs_l = wavs[:,:,0]
    wavs_r = wavs[:,:,1]
    wavs_l = resampler(wavs_l)
    wavs_r = resampler(wavs_r)
    return wavs_l,wavs_r


def audio_pipeline(path,fs=32000):
    wavs = sb.dataio.dataio.read_audio_multichannel(path)    
    return wavs


def format_correctness(y):
    y = torch.tensor([y])
    y = y/100
    return y


def format_feats(y):
    y = torch.from_numpy(y).to(torch.float)
    return y[0]


def get_stats(predictions, correctness):
    rmse = rmse_score(predictions, correctness)
    p_corr = np.corrcoef(predictions,correctness)[0][1]
    s_corr = spearmanr(predictions,correctness)[0]
    std = std_err(predictions, correctness)
    stats_dict = {
        "rmse": rmse,
        "p_corr": p_corr,
        "s_corr": s_corr,
        "std": std
    }
    return stats_dict


def get_dis_val_set_losses(dis_val_preds, correct, validation):
    
    # 1: disjoint validation set (listener) note: actually [1, 3, 5, 7]
    # 2: disjoint validation set (system) note: actually [2, 3, 6, 7]
    # 3: disjoint validation set (listener, system) note: actually [3, 7]
    # 4: disjoint validation set (scene) note: actually [4, 5, 6, 7]
    # 5: disjoint validation set (listener, scene) note: actually [5, 7]
    # 6: disjoint validation set (system, scene) note: actually [6, 7]
    # 7: disjoint validation set (listener, system, scene)

    stats = [0] * 4
    vals = [[7],        
        [1, 3, 5, 7], 
        [2, 3, 6, 7], 
        [4, 5, 6, 7]]
    
    dis_val_preds = torch.tensor(dis_val_preds)
    correct = torch.tensor(correct)
    validation = torch.tensor(validation)

    for i in range(len(stats)):
        dis_val_bool = torch.zeros(len(dis_val_preds), dtype = torch.bool)
        dis_val_bool[torch.isin(validation, torch.tensor(vals[i]))] = True

        stats[i] = get_stats(dis_val_preds[dis_val_bool].numpy(), correct[dis_val_bool].numpy())

    return stats[0], stats[1], stats[2], stats[3]


def extract_feats(feat_extractor, data, args, theset, save_feats_file = None):
    feat_extractor.eval()
    name_list = data["signal"]
    correctness_list = data["correctness"]
    listener_list = data["listener"]
    scene_list = data["scene"]
    subset_list = data["subset"]

    feat_extractor.eval()
    feat_extractor.to(args.device)

    test_dict = {}
    
    for sub,name,corr,scene,lis in zip(subset_list,name_list,correctness_list,scene_list,listener_list):
        test_dict[name] = {"subset":sub,"signal": name,"correctness":corr,"scene": scene,"listener":lis}
    
    dynamic_items = [
        {"func": lambda l: format_correctness(l),
        "takes": "correctness",
        "provides": "formatted_correctness"},
        
    ]
    if args.use_CPC1:
        print(args.dataroot,theset)
        input()
        dynamic_items.append(
            {"func": lambda l: audio_pipeline("%s/HA_outputs/%s/%s.wav"%(args.dataroot,theset,l),32000),
            "takes": ["signal"],
            "provides": "wav"}
        )
    else:

        print(f"{args.dataroot}/HA_outputs/signals/%s.wav")
        input()
        dynamic_items.append(
            {"func": lambda l,y: audio_pipeline(f"{args.dataroot}/HA_outputs/signals/{y}/{l}.wav",32000),
            "takes": ["signal","subset"],
            "provides": "wav"}
        )

    data_set = sb.dataio.dataset.DynamicItemDataset(test_dict,dynamic_items)
    data_set.set_output_keys(["wav", "formatted_correctness"])

    feats_list_l = []
    feats_list_r = []
    correct_list = []
    
    resampler = T.Resample(32000, 16000).to(args.device)

    my_dataloader = DataLoader(data_set,1,collate_fn=sb.dataio.batch.PaddedBatch)
    print("Extracting features...")
    for batch in tqdm(my_dataloader):

        batch = batch.to(args.device)
        wavs, correctness = batch
        correctness = correctness.data

        wavs_data = wavs.data
        feats_l,feats_r = compute_feats(wavs_data,resampler)
       
        extracted_feats_l = feat_extractor(feats_l.float())
        extracted_feats_r = feat_extractor(feats_r.float())

        feats_list_l.append(extracted_feats_l.detach().cpu().numpy())
        feats_list_r.append(extracted_feats_r.detach().cpu().numpy())
        correct_list.append(correctness)

    data['feats_l'] = feats_list_l
    data['feats_r'] = feats_list_r

    # print(f"size 0 l: {feats_list_l[0]}")
    # print(f"size 0 r: {feats_list_r[0]}")
    print(f"size 0 l: {feats_list_l[0].shape}")
    print(f"size 0 r: {feats_list_r[0].shape}")
    # print(f"one layer size 0 l: {feats_list_l[0][:, :, :, 0].shape}")
    # print(f"one layer size 0 r: {feats_list_r[0][:, :, :, 0].shape}")


    if save_feats_file is not None:
        if args.layer == -1:
            for layer in range(args.num_layers):
                # print(f"{save_feats_file}{layer}_l.txt")
                with open(f"{save_feats_file}{layer}_l.txt", "w") as f:
                    f.write("\n".join(" ".join(map(str, feats[:, :, :, layer].flatten())) for feats in feats_list_l))
                with open(f"{save_feats_file}{layer}_r.txt", "w") as f:
                    f.write("\n".join(" ".join(map(str, feats[:, :, :, layer].flatten())) for feats in feats_list_r))

        else:
            print(f"{save_feats_file}_l.txt")
            with open(f"{save_feats_file}_l.txt", "w") as f:
                f.write("\n".join(" ".join(map(str, feats.flatten())) for feats in feats_list_l))
            with open(f"{save_feats_file}_r.txt", "w") as f:
                f.write("\n".join(" ".join(map(str, feats.flatten())) for feats in feats_list_r))

    return data


def get_dynamic_dataset(data):

    name_list = data["signal"]
    correctness_list = data["correctness"]
    scene_list = data["scene"]
    listener_list = data["listener"]
    subset_list = data["subset"]
    feats_l_list = data["feats_l"]
    feats_r_list = data["feats_r"]
   
    data_dict = {}
    for sub,name,corr,scene,lis, f_l, f_r in  zip(subset_list,name_list,correctness_list,scene_list,listener_list, feats_l_list, feats_r_list):
        data_dict[name] = {"subset":sub,"signal": name,"correctness":corr,"scene": scene,"listener":lis, "feats_l":f_l, "feats_r": f_r}
    

        dynamic_items = [
            {"func": lambda l: format_correctness(l),
            "takes": "correctness",
            "provides": "formatted_correctness"},
            {"func": lambda l: format_feats(l),
            "takes": "feats_l",
            "provides": "formatted_feats_l"},
            {"func": lambda l: format_feats(l),
            "takes": "feats_r",
            "provides": "formatted_feats_r"}
        ]

    ddata = sb.dataio.dataset.DynamicItemDataset(data_dict,dynamic_items)
    ddata.set_output_keys(["formatted_correctness", "formatted_feats_l", "formatted_feats_r"])

    return ddata


def validate_model(model,test_data,optimizer,criterion,args,ex_data = None):
    out_list = []
    model.eval()
    running_loss = 0.0
    loss_list = []

    test_set = get_dynamic_dataset(test_data)
    my_dataloader = DataLoader(test_set,args.batch_size,collate_fn=sb.dataio.batch.PaddedBatch, shuffle = False)

    # Get exemplar data loader if using exemplars
    if ex_data is not None:
        if args.random_exemplars:
            len_ex = len(test_data)
            ex_set = get_dynamic_dataset(test_data)
            ex_dataloader = DataLoader(ex_set,args.ex_size*args.num_minervas,collate_fn=sb.dataio.batch.PaddedBatch, shuffle = True)
        else:
            ex_set = get_ex_set(ex_data, args)
            len_ex = len(ex_set)
            ex_set = get_dynamic_dataset(ex_set)
            ex_dataloader = DataLoader(ex_set,args.ex_size*args.num_minervas,collate_fn=sb.dataio.batch.PaddedBatch, shuffle = False)
        ex_used = 0
        ex_dataloader = iter(ex_dataloader)

    print("starting validation...")
    for batch in tqdm(my_dataloader):
       
        correctness, feats_l, feats_r = batch

        # Use packed sequences for batches
        feats_l = torch.nn.utils.rnn.pack_padded_sequence(
            feats_l.data, 
            (feats_l.lengths * feats_l.data.size(1)).to(torch.int64), 
            batch_first=True,
            enforce_sorted = False
        )
        feats_r = torch.nn.utils.rnn.pack_padded_sequence(
            feats_r.data, 
            (feats_r.lengths * feats_r.data.size(1)).to(torch.int64), 
            batch_first=True,
            enforce_sorted = False
        )
        feats_l = feats_l.to(args.device)
        feats_r = feats_r.to(args.device)

        # Get exemplar for this minibatch, if using exemplar model
        if ex_data is not None:
            ex_used += args.num_minervas * args.ex_size
            if ex_used > len_ex:
                if args.random_exemplars:
                    ex_dataloader = iter(DataLoader(ex_set,args.ex_size*args.num_minervas, collate_fn=sb.dataio.batch.PaddedBatch, shuffle = True))
                else:
                    ex_set = get_ex_set(ex_data, args)
                    ex_set = get_dynamic_dataset(ex_data)
                    ex_dataloader = iter(DataLoader(ex_set,args.ex_size*args.num_minervas, collate_fn=sb.dataio.batch.PaddedBatch, shuffle = False))
                ex_used = args.num_minervas * args.ex_size

            exemplars = next(ex_dataloader)
            ex_correct, ex_feats_l, ex_feats_r = exemplars
            # print(f"ex_correct:\n{ex_correct}")
            ex_feats_l = torch.nn.utils.rnn.pack_padded_sequence(
                ex_feats_l.data, 
                (ex_feats_l.lengths * ex_feats_l.data.size(1)).to(torch.int64), 
                batch_first=True,
                enforce_sorted = False
            )
            ex_feats_r = torch.nn.utils.rnn.pack_padded_sequence(
                ex_feats_r.data, 
                (ex_feats_r.lengths * ex_feats_r.data.size(1)).to(torch.int64), 
                batch_first=True,
                enforce_sorted = False
            )
            ex_feats_l = ex_feats_l.to(args.device)
            ex_feats_r = ex_feats_r.to(args.device)
            ex_correct = ex_correct.data.to(args.device)
            
        target_scores = correctness.data.to(args.device)

        if ex_data is not None:
            output_l, _ = model(feats_l, ex_feats_l, ex_correct, num_minervas = args.num_minervas)
            output_r, _ = model(feats_r, ex_feats_r, ex_correct, num_minervas = args.num_minervas)
        else:
            output_l,_ = model(feats_l)
            output_r,_ = model(feats_r)

        if args.learn_incorrect:
            output = torch.minimum(output_l,output_r)
        else:
            output = torch.maximum(output_l,output_r)
        loss = criterion(output,target_scores)

        for out_val in output:
            # print(out_val)
            out_list.append(out_val.detach().cpu().numpy()[0]*100)

        loss_list.append(loss.item())
        # print statistics
        running_loss += loss.item()
    return out_list,sum(loss_list)/len(loss_list)

def get_feats(data, save_feats_file, dim_extractor, feat_extractor, theset, args):
    
    if args.extract_feats:
        files_exist = False
    elif args.layer == -1:
        files_exist = True
        for layer in range(args.num_layers):
            files_exist = files_exist if os.path.exists(f"{save_feats_file}{layer}_l.txt") else False
            files_exist = files_exist if os.path.exists(f"{save_feats_file}{layer}_r.txt") else False
    else:
        files_exist = os.path.exists(save_feats_file + "_l.txt") and os.path.exists(save_feats_file + "_r.txt")
    
    # Load feats from file if they exist (only works for Whisper decoder)
    if args.layer != -1 and files_exist:
        print("Loading feats from file...")
        
        with open(save_feats_file + "_l.txt", "r") as f:
            feats_l = f.readlines()
        with open(save_feats_file + "_r.txt", "r") as f:
            feats_r = f.readlines()

        feats_l = [np.fromstring(feats, dtype=float, sep=' ') for feats in feats_l]
        feats_r = [np.fromstring(feats, dtype=float, sep=' ') for feats in feats_r]
        feats_l = [feats.reshape(1, -1, dim_extractor) for feats in feats_l]
        feats_r = [feats.reshape(1, -1, dim_extractor) for feats in feats_r]

        data['feats_l'] = feats_l
        data['feats_r'] = feats_r

    elif args.layer == -1 and files_exist:
        print("Loading feats from file...")
        
        feats_l = []
        feats_r = []
        for layer in range(args.num_layers):
            print(f"Loading layer {layer}...")
            with open(f"{save_feats_file}{layer}_l.txt", "r") as f:
                feats_l_layer = f.readlines()
            with open(f"{save_feats_file}{layer}_r.txt", "r") as f:
                feats_r_layer = f.readlines()
            feats_l.append([np.fromstring(feats, dtype=float, sep=' ') for feats in feats_l_layer])
            feats_r.append([np.fromstring(feats, dtype=float, sep=' ') for feats in feats_r_layer])
            feats_l[layer] = [feats.reshape(1, -1, dim_extractor) for feats in feats_l[layer]]
            feats_r[layer] = [feats.reshape(1, -1, dim_extractor) for feats in feats_r[layer]]

        feats_l_all = []
        feats_r_all = []
        for row in range(len(feats_l[0])):
            temp_feats_l = []
            temp_feats_r = []
            for layer in range(len(feats_l)):
                temp_feats_l.append(feats_l[layer][row])
                temp_feats_r.append(feats_r[layer][row])
            feats_l_all.append(np.stack(temp_feats_l, axis = -1))
            feats_r_all.append(np.stack(temp_feats_r, axis = -1))

        data['feats_l'] = feats_l_all
        data['feats_r'] = feats_r_all
        # feats_l = np.stack([feats_l[layer][row] for row in feats_l[layer] for layer in len(feats_l)])

    else:
        # Extract feats
        data = extract_feats(
            feat_extractor, 
            data, 
            args, 
            theset, 
            save_feats_file = save_feats_file if args.save_feats else None
        )
        feat_extractor.to('cpu')

    return data


def format_audiogram(audiogram):
    """
     "audiogram_cfs": [
      250,
      500,
      1000,
      2000,
      3000,
      4000,
      6000,
      8000
    ],
    
    TO
    [250, 500, 1000, 2000, 4000, 6000]
    """
    audiogram = numpy.delete(audiogram,4)
    audiogram = audiogram[:-1]
    return audiogram
    

def train_model(model,train_data,optimizer,criterion,args,ex_data=None):
    model.train()

    # print(ex_data)
        
    running_loss = 0.0
    loss_len = 0

    train_set = get_dynamic_dataset(train_data)
    my_dataloader = DataLoader(train_set,args.batch_size,collate_fn=sb.dataio.batch.PaddedBatch, shuffle = True)

    # Set up exemplar data loader if using exemplars
    if ex_data is not None:
        if args.random_exemplars:
            ex_set = get_dynamic_dataset(ex_data)
            len_ex = len(ex_set)
            ex_dataloader = DataLoader(ex_set,args.ex_size*args.num_minervas,collate_fn=sb.dataio.batch.PaddedBatch, shuffle = True)
        else:
            ex_set = get_ex_set(ex_data, args)
            len_ex = len(ex_set)
            ex_set = get_dynamic_dataset(ex_set)
            ex_dataloader = DataLoader(ex_set,args.ex_size*args.num_minervas,collate_fn=sb.dataio.batch.PaddedBatch, shuffle = False)
            # len_ex = len(ex_dataloader)
        ex_used = 0
        ex_dataloader = iter(ex_dataloader)

    print(f"batch_size: {args.batch_size}")
    print("starting training...")
    
    for batch in tqdm(my_dataloader, total=len(my_dataloader)):

        correctness, feats_l, feats_r = batch

        # Use packed sequences for variable-length
        # (not needed for Whisper, but needed for some others)
        feats_l = torch.nn.utils.rnn.pack_padded_sequence(
            feats_l.data, 
            (feats_l.lengths * feats_l.data.size(1)).to(torch.int64), 
            batch_first=True,
            enforce_sorted = False
        )
        feats_r = torch.nn.utils.rnn.pack_padded_sequence(
            feats_r.data, 
            (feats_r.lengths * feats_r.data.size(1)).to(torch.int64), 
            batch_first=True,
            enforce_sorted = False
        )
        feats_l = feats_l.to(args.device)
        feats_r = feats_r.to(args.device)

        # Get the exemplars for the minibatch, if using exemplar model
        if ex_data is not None:
            # print(f"ex_used: {ex_used}, len_ex: {len_ex}")
            ex_used += args.ex_size * args.num_minervas
            if ex_used > len_ex:
                if args.random_exemplars:
                    ex_dataloader = iter(DataLoader(ex_set,args.ex_size * args.num_minervas,collate_fn=sb.dataio.batch.PaddedBatch, shuffle = True))
                else:
                # print("Reloading exemplars...")
                    ex_set = get_ex_set(ex_data, args)
                    ex_set = get_dynamic_dataset(ex_set)
                    ex_dataloader = iter(DataLoader(ex_set,args.ex_size * args.num_minervas,collate_fn=sb.dataio.batch.PaddedBatch, shuffle = False))
                ex_used = args.ex_size * args.num_minervas
                
            # print(f"ex_used: {ex_used}, len_ex: {len_ex}")

            exemplars = next(ex_dataloader)
            # print(exemplars)
            ex_correct, ex_feats_l, ex_feats_r = exemplars
            # print(f"ex_correct:\n{ex_correct}")
            # print(f"\nexemplar correctness:\n{ex_correct}\n")
            ex_feats_l = torch.nn.utils.rnn.pack_padded_sequence(
                ex_feats_l.data, 
                (ex_feats_l.lengths * ex_feats_l.data.size(1)).to(torch.int64), 
                batch_first=True,
                enforce_sorted = False
            )
            ex_feats_r = torch.nn.utils.rnn.pack_padded_sequence(
                ex_feats_r.data, 
                (ex_feats_r.lengths * ex_feats_r.data.size(1)).to(torch.int64), 
                batch_first=True,
                enforce_sorted = False
            )
            ex_feats_l = ex_feats_l.to(args.device)
            ex_feats_r = ex_feats_r.to(args.device)
            ex_correct = ex_correct.data.to(args.device)

        target_scores = correctness.data.to(args.device)
        
        optimizer.zero_grad()

        if ex_data is not None:
            output_l, _ = model(feats_l, ex_feats_l, ex_correct, num_minervas = args.num_minervas)
            output_r, _ = model(feats_r, ex_feats_r, ex_correct, num_minervas = args.num_minervas)
        else:
            output_l,_ = model(feats_l)
            output_r,_ = model(feats_r)
        loss_l = criterion(output_l,target_scores)
        loss_r = criterion(output_r,target_scores)

        # Sum the right and left losses - note that training loss is
        # therefore doubled compared to evaluation loss
        loss = loss_l + loss_r

        loss.backward()
        if args.grad_clipping is not None:
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clipping)
        optimizer.step()
        running_loss += loss.item()
        loss_len += 1

    if args.model == "LSTM_layers" or args.model == "ExLSTM_layers":
        print(f"\nlayer weights sm:\n{model.sm(model.layer_weights)}\n")
        print(f"\nlayer weights:\n{model.layer_weights}\n")
        print(f"layer weights sum: {model.sm(model.layer_weights).sum().item()}\n")
        
    return model, optimizer, criterion, running_loss / (2 * loss_len)


def save_model(model,opt,epoch,args,val_loss):
    p = args.model_dir
    if not os.path.exists(p):
        os.mkdir(p)
    m_name = "%s-%s"%(args.model,args.seed)
    torch.save(model.state_dict(),"%s/%s_%s_%s_model.pt"%(p,m_name,epoch,val_loss))
    torch.save(opt.state_dict(),"%s/%s_%s_%s_opt.pt"%(p,m_name,epoch,val_loss))


def main(args, config):
    #set up the torch objects
    print("creating model: %s"%args.feats_model)
    torch.backends.cudnn.deterministic = True
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)
    torch.cuda.manual_seed(args.seed)

    args.exemplar = False

    # Select a pretrained feature extractor model
    if args.feats_model == "XLSREncoder":
        feat_extractor = XLSREncoder_feats()
        dim_extractor = 512
        hidden_size = 512//2
        activation = nn.LeakyReLU
        att_pool_dim = 512

    elif args.feats_model == "XLSRFull":
        feat_extractor = XLSRFull_feats()
        dim_extractor = 1024
        hidden_size = 1024//2
        activation = nn.LeakyReLU
        att_pool_dim = 1024
        
    elif args.feats_model == "XLSRCombo":
        ## Tricky one! Check fat extractor
        print("XLSRCombo Not implemented.")
        quit()
        # combo = True
        # feat_extractor = XLSRCombo_feats()
        # dim_extractor = 1024
        # hidden_size = 1024//2
        # activation = nn.LeakyReLU
        
    elif args.feats_model == "HuBERTEncoder":
        feat_extractor = HuBERTEncoder_feats()
        dim_extractor = 512
        hidden_size = 512//2
        activation = nn.LeakyReLU
        att_pool_dim = 512
        
    elif args.feats_model == "HuBERTFull":
        feat_extractor = HuBERTFull_feats()
        dim_extractor = 768
        hidden_size = 768//2
        activation = nn.LeakyReLU
        att_pool_dim = 768
        
    elif args.feats_model == "Spec":  
        feat_extractor = Spec_feats()
        dim_extractor = 257
        hidden_size = 257//2
        activation = nn.LeakyReLU
        att_pool_dim = 256

    elif args.feats_model == "WhisperEncoder":  
        feat_extractor = WhisperEncoder_feats(
            pretrained_model=args.pretrained_feats_model, 
            use_feat_extractor = True,
            layer = args.layer
        )
        dim_extractor = 768
        hidden_size = 768//2
        activation = nn.LeakyReLU
        att_pool_dim = 768

    elif args.feats_model == "WhisperFull":  
        feat_extractor = WhisperFull_feats(
            pretrained_model=args.pretrained_feats_model, 
            use_feat_extractor = True,
            layer = args.layer
        )
        dim_extractor = 768
        hidden_size = 768//2
        activation = nn.LeakyReLU
        att_pool_dim = 768
        
    elif args.feats_model == "WhisperBase":  
        feat_extractor = WhisperBase_feats(
            pretrained_model=args.pretrained_feats_model, 
            use_feat_extractor = True,
            layer = args.layer
        )
        dim_extractor = 512
        hidden_size = 512//2
        activation = nn.LeakyReLU
        att_pool_dim = 512
        
    else:
        print("Feats extractor not recognised")
        exit(1)

    # Select classifier model (this is the one we're actually training)
    if args.model == "LSTM":
        model = MetricPredictorLSTM(dim_extractor, hidden_size, activation, att_pool_dim)
    elif args.model == "LSTM_layers":
        model = MetricPredictorLSTM_layers(dim_extractor, hidden_size, activation, att_pool_dim, num_layers = args.num_layers)
    elif args.model == "AttenPool":
        model = MetricPredictorAttenPool(att_pool_dim)
    elif args.model == "ExLSTM":
        model = ExLSTM(dim_extractor, hidden_size, att_pool_dim, p_factor = args.p_factor, minerva_dim = args.minerva_dim)
        args.exemplar = True
    elif args.model == "ExLSTM_layers":
        model = ExLSTM_layers(
            dim_extractor, 
            hidden_size, 
            att_pool_dim, 
            p_factor = args.p_factor, 
            minerva_dim = args.minerva_dim, 
            minerva_R_dim = args.minerva_r_dim,
            num_layers = args.num_layers,
            use_r_encoding = args.use_r_encoding
        )
        args.exemplar = True
    elif args.model == "ExLSTM_log":
        model = ExLSTM_log(dim_extractor, hidden_size, att_pool_dim, p_factor = args.p_factor, minerva_dim = args.minerva_dim, num_layers = args.num_layers)
        args.exemplar = True
    elif args.model == "ExLSTM_div":
        model = ExLSTM_div(dim_extractor, hidden_size, att_pool_dim, p_factor = args.p_factor, minerva_dim = args.minerva_dim, num_layers = args.num_layers)
        args.exemplar = True
    elif args.model == "wordLSTM":
        model = wordLSTM(dim_extractor, hidden_size, att_pool_dim, num_layers = args.num_layers)
        args.exemplar = True
        
    elif args.model == "ExLSTM_std":
        model = ExLSTM_std(
            dim_extractor, 
            hidden_size, 
            att_pool_dim, 
            p_factor = args.p_factor, 
            minerva_dim = args.minerva_dim, 
            num_minervas = args.num_minervas,
            num_layers = args.num_layers)
        args.exemplar = True
    else:
        print("Model not recognised")
        exit(1)

    # If restarting a training run, load the weights
    if args.restart_model is not None:
        model.load_state_dict(torch.load(args.restart_model))

    # Set up logging on WandB (handy for keeping an eye on things)
    if not args.skip_wandb:
        # wandb_name = "%s_%s_%s_%s_feats_%s_%s"%(args.exp_id,args.N,args.model,ex,date,args.seed)
        run = wandb.init(
            project=args.wandb_project, 
            reinit = True, 
            name = args.model_name,
            tags = [f"N{args.N}", f"lr{args.lr}", args.feats_model, args.model, f"bs{args.batch_size}"]
        )
        if args.exemplar:
            run.tags = run.tags + ("exemplar", f"es{args.ex_size}{args.exemplar_source}", f"p{args.p_factor}")
        if args.restart_model is not None:
            run.tags = run.tags + ("resumed", )
        if args.layer is not None:
            run.tags = run.tags + (f"layer{args.layer}", )
        if args.minerva_dim is not None:
            run.tags = run.tags + (f"min_dim{args.minerva_dim}", )
        if args.random_exemplars:
            run.tags = run.tags + (f"random_ex", )
        else:
            run.tags = run.tags + (f"strat_ex", )
        if args.num_minervas != 1:
            run.tags = run.tags + (f"nm{args.num_minervas}", )
        if args.learn_incorrect:
            run.tags = run.tags + (f"learn_incorrect", )
        if args.minerva_r_dim is not None:
            run.tags = run.tags + (f"r_dim{args.minerva_r_dim}", )
        if args.use_r_encoding:
            run.tags = run.tags + (f"r_encoding", )
        if args.train_disjoint:
            run.tags = run.tags + (f"train_disjoint", )





    # Make a model directory (if it doesn't exist) and write out config for future reference
    # args.model_dir = model_dir
    if not os.path.exists(args.model_dir):
        os.mkdir(args.model_dir)
    with open(args.model_dir + "/config.json", 'w+') as f:
        f.write(json.dumps(dict(config)))
        
    criterion = nn.MSELoss()

    optParams = []
    for param_name, param in model.named_parameters():
        if param_name == "layer_weights":
            optParams.append(
                {'params': param, 'weight_decay': 0}
            )
        else:
            optParams.append(
                {'params': param, 'weight_decay': args.weight_decay}
            )


    optimizer = optim.Adam(optParams,lr=args.lr)
    if not args.use_CPC1:
        if int(args.in_json_file.split("/")[-1].split(".")[-2]) != int(args.N):
            print("Warning: N does not match dataset:")
            print(args.in_json_file.split("/")[-1].split(".")[-2],args.N)
            exit()

    # You can save the feats extracted to disk so it doesn't have to be done 
    # again for future test runs (takes up space though)
    pre = "" if args.pretrained_feats_model is None else "pre"
    save_feats_file = f"{args.dataroot}{args.feats_model}{pre}_N{args.N}_{'debug_' if args.debug else ''}{'' if args.layer == -1 else args.layer}"
    print(f"save_feats_file:\n{save_feats_file}")
    data = pd.read_json(args.in_json_file)
    data["subset"] = "CEC1"
    if not args.use_CPC1:
        data2 = pd.read_json(args.in_json_file.replace("CEC1","CEC2"))
        data2["subset"] = "CEC2"
        data = pd.concat([data, data2])
    data = data.drop_duplicates(subset = ['signal'], keep = 'last')
    # print(data["correctness"])
    if args.learn_incorrect:
        data["correctness"] = 100 - data["correctness"].to_numpy()
    data["predicted"] = np.nan  

    if args.debug:
        _, data = train_test_split(data, test_size = 0.01)

    theset = "train_indep" if args.use_CPC1 else "train"
    data = get_feats(
        data = data,
        save_feats_file = save_feats_file,
        dim_extractor = dim_extractor,
        feat_extractor = feat_extractor,
        theset = theset,
        args = args
    )


    # Reset seeds for repeatable behaviour regardless of
    # how feats / models are obtained
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)
    torch.cuda.manual_seed(args.seed)

    # Mark various disjoint validation sets in the data
    data = get_disjoint_val_set(args, data)
    dis_val_data = data[data.validation > 0].copy()
    train_data,val_data = train_test_split(data[data.validation == 0],test_size=0.1)

    # Split into train and val sets
    if args.train_disjoint:
        print("Training on disjoint")
        train_data = pd.concat([train_data, dis_val_data])
    # print(dis_val_data["correctness"])
    # print(train_data["correctness"])
    # print(val_data["correctness"])
    # quit()

    print("Trainset: %s\nValset: %s\nDisValset: %s"%(train_data.shape[0],val_data.shape[0],dis_val_data.shape[0]))
    print("=====================================")

    if args.exemplar:
        if args.exemplar_source == "CEC1":
            train_ex_data = train_data[train_data.subset == "CEC1"]
            dis_val_ex_data = train_data[train_data.subset == "CEC1"]
        elif args.exemplar_source == "CEC2":
            train_ex_data = train_data[train_data.subset == "CEC2"]
            dis_val_ex_data = train_data[train_data.subset == "CEC2"]
        elif args.exemplar_source == "matching":
            train_ex_data = train_data
            dis_val_ex_data = train_data[train_data.subset == "CEC2"]
        elif args.exemplar_source == "all":
            train_ex_data = train_data
            dis_val_ex_data = train_data
        else:
            print("Invalid exemplar exemplar source.")
            quit()
        print("Using training data for exemplars")
    else:
        train_ex_data = None
        dis_val_ex_data = None

    train_data = train_data.sample(frac=1).reset_index(drop=True)

    model = model.to(args.device)

    if not args.skip_train:
        print("Starting training of model: %s\nlearning rate: %s\nseed: %s\nepochs: %s\nsave location: %s/"%(args.model,args.lr,args.seed,args.n_epochs,args.model_dir))
        print("=====================================")
        for epoch in range(args.restart_epoch, args.n_epochs + args.restart_epoch):

            model,optimizer,criterion,training_loss = train_model(model,train_data,optimizer,criterion,args,ex_data=train_ex_data)

            val_predictions, val_loss = validate_model(model,val_data,optimizer,criterion,args,train_ex_data)
            preds, _ = validate_model(model,dis_val_data,optimizer,criterion,args,dis_val_ex_data)

            # Get val loss from predictions (minibatching may cause rounding errors)
            val_stats = get_stats(val_predictions, val_data.correctness.values)
            # Get losses for the various disjoint subsets held within dis_val_data
            dis_val, dis_lis_val, dis_sys_val, dis_scene_val = get_dis_val_set_losses(
                dis_val_preds = preds,
                correct = dis_val_data["correctness"].values,
                validation = dis_val_data["validation"].values
            )

            if not args.skip_wandb:
                log_dict = {
                    "val_rmse": val_stats['rmse'],
                    "dis_val_rmse": dis_val['rmse'],
                    "dis_lis_val_rmse": dis_lis_val['rmse'],
                    "dis_sys_val_rmse": dis_sys_val['rmse'],
                    "dis_scene_val_rmse": dis_scene_val['rmse'],
                    "train_rmse": (training_loss**0.5)*100
                }
                
                wandb.log(log_dict)
            
            save_model(model,optimizer,epoch,args,val_loss)
            torch.cuda.empty_cache()
            print("Epoch: %s"%(epoch))
            print("\tTraining Loss: %s"%((training_loss**0.5)*100))
            print("\tValidation Loss: %s"%(val_stats['rmse']))
            print("\tDisjoint validation Loss: %s"%(dis_val['rmse']))
            print("\tDisjoint listener validation Loss: %s"%(dis_lis_val['rmse']))
            print("\tDisjoint system validation Loss: %s"%(dis_sys_val['rmse']))
            print("\tDisjoint scene validation Loss: %s"%(dis_scene_val['rmse']))
            print("=====================================")    

            if args.model == "LSTM_layers" or args.model == "ExLSTM_layers":
                with open(f"{args.model_dir}/layer_weights.txt", 'a') as f:
                    f.write(" ".join([str(weight) for weight in model.sm(model.layer_weights).tolist()]))
                    f.write("\n")


    if args.skip_train and args.pretrained_model_dir is not None:
        model_dir = args.pretrained_model_dir
    else:
        model_dir = args.model_dir

    print(model_dir)
    model_files = os.listdir(model_dir)

    model_files = [x for x in model_files if "model" in x]
    model_files.sort(key=lambda x: float(x.split("_")[-2].strip(".pt")))
    model_file = model_files[0]
    print("Loading model:\n%s"%model_file)
    model.load_state_dict(torch.load("%s/%s"%(model_dir,model_file)))
    
    # get validation predictions
    val_predictions,val_loss = validate_model(model,val_data,optimizer,criterion,args,train_ex_data)
    val_stats = get_stats(val_predictions, val_data.correctness.values)
    val_data["predicted"] = val_predictions
    val_data[["scene", "listener", "system", "correctness", "predicted"]].to_csv(f"{args.out_csv_file}_val_preds.csv", index=False)

    dis_val_predictions, _ = validate_model(model,dis_val_data,optimizer,criterion,args,dis_val_ex_data)
    dis_val_data["predicted"] = dis_val_predictions

    # Write a csv file of restults for each data split:
    dis_val_data[["scene", "listener", "system", "validation", "correctness", "predicted"]].to_csv(f"{args.out_csv_file}_all_dis_val_preds.csv", index=False)
    temp_data = dis_val_data[dis_val_data.validation == 7].copy()
    temp_data[["scene", "listener", "system", "correctness", "predicted"]].to_csv(f"{args.out_csv_file}_dis_val_preds.csv", index=False)
    temp_data = dis_val_data[dis_val_data.validation.isin([1, 3, 5, 7])].copy()
    temp_data[["scene", "listener", "system", "correctness", "predicted"]].to_csv(f"{args.out_csv_file}_dis_lis_val_preds.csv", index=False)
    temp_data = dis_val_data[dis_val_data.validation.isin([2, 3, 6, 7])].copy()
    temp_data[["scene", "listener", "system", "correctness", "predicted"]].to_csv(f"{args.out_csv_file}_dis_sys_val_preds.csv", index=False)
    temp_data = dis_val_data[dis_val_data.validation.isin([4, 5, 6, 7])].copy()
    temp_data[["scene", "listener", "system", "correctness", "predicted"]].to_csv(f"{args.out_csv_file}_dis_scene_val_preds.csv", index=False)

    dis_val, dis_lis_val, dis_sys_val, dis_scene_val = get_dis_val_set_losses(
        dis_val_preds = dis_val_predictions,
        correct = dis_val_data["correctness"].values,
        validation = dis_val_data["validation"].values
    )
    
    #normalise the predictions
    val_gt = val_data["correctness"].to_numpy()/100
    val_predictions = np.asarray(val_predictions)/100

    def logit_func(x,a,b):
     return 1/(1+np.exp(a*x+b))

    # logistic mapping curve fit to get the a and b parameters
    popt,_ = curve_fit(logit_func, val_predictions, val_gt)
    a,b = popt
    print("a: %s b: %s"%(a,b))
    print("=====================================")
    # Test the model
    if not args.skip_test:
        print("Testing model on test set")
        test_data = pd.read_json(args.test_json_file)
        test_data["subset"] = "CEC2"
        save_feats_file = f"{args.dataroot}{args.feats_model}{pre}_N{args.N}_{'debug_' if args.debug else ''}{'' if args.layer == -1 else args.layer}_test"

        test_data = get_feats(
            data = test_data,
            save_feats_file = save_feats_file,
            dim_extractor = dim_extractor,
            feat_extractor = feat_extractor,
            theset = "test",
            args = args
        )

        predictions, _ = validate_model(model, test_data, optimizer, criterion, args, dis_val_ex_data)
        test_stats = get_stats(predictions, test_data.correctness.values)
        predictions_fitted = np.asarray(predictions)/100
        #apply the logistic mapping
        predictions_fitted = logit_func(predictions_fitted,a,b)
        test_data["predicted"] = predictions
        test_data["predicted_fitted"] = predictions_fitted * 100
        test_data[["signal", "scene", "listener", "system", "predicted", "predicted_fitted"]].to_csv(f"{args.out_csv_file}_test_preds.csv", index=False)


    # Write the final stastistics to the summary file
    with open (args.summ_file, "a") as f:
        csv_writer = csv.writer(f)
        csv_writer.writerow([
            args.out_csv_file.split("/")[-1].strip(".csv"), 
            val_stats['rmse'], val_stats["p_corr"], val_stats["s_corr"], val_stats["std"], 
            dis_val["rmse"], dis_val["p_corr"], dis_val["s_corr"], dis_val["std"], 
            dis_lis_val["rmse"], dis_lis_val["p_corr"], dis_lis_val["s_corr"], dis_lis_val["std"], 
            dis_sys_val["rmse"], dis_sys_val["p_corr"], dis_sys_val["s_corr"], dis_sys_val["std"], 
            dis_scene_val["rmse"], dis_scene_val["p_corr"], dis_scene_val["s_corr"], dis_scene_val["std"],
            test_stats["rmse"], test_stats["p_corr"], test_stats["s_corr"], test_stats["std"]
        ])
    
    print("=====================================")

    if not args.skip_wandb:
        run.finish()

if __name__ == "__main__":
    print("CUDA available = ", torch.cuda.is_available())
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--config_file", help="location of configuation json file", default = None #"configs/default_config.json"
    )
    parser.add_argument(
        "--exp_id", help="id for individual experiment", default = None
    )
    parser.add_argument(
        "--summ_file", help="path to write summary results to" , default="save/CPC_metrics.csv"
    )
    parser.add_argument(
        "--out_csv_file", help="path to write the preditions to" , default=None
    )
    parser.add_argument(
        "--wandb_project", help="W and B project name" , default="CPC"
    )
    parser.add_argument(
        "--skip_wandb", help="skip logging via WandB", default=False, action='store_true'
    )
    parser.add_argument(
        "--seed", help="random seed for repeatability", default=1234, type=int
    )
    parser.add_argument(
        "--debug", help="use a tiny dataset for debugging", default=False, action='store_true'
    )
    parser.add_argument(
        "--save_feats", help="save extracted feats to disk for future use", default=False, action='store_true'
    )
    parser.add_argument(
        "--extract_feats", help="extract feats rather than loading from file", default=False, action='store_true'
    )
    parser.add_argument(
        "--pretrained_model_dir", help="directory of pretrained model", default=None
    )
    parser.add_argument(
        "--skip_test", help="do predictions on test data", default=False, action='store_true'
    )


    # Training data
    parser.add_argument(
        "--skip_train", help="do training", default=False, action='store_true'
    )
    parser.add_argument(
        "--use_CPC1", help="train and evaluate on CPC1 data" , default=False, action='store_true'
    )
    parser.add_argument(
        "--N", help="train split" , default=1, type=int
    )
    parser.add_argument(
        "--learn_incorrect", help="predict incorrectness (100 - correctness) when training", default=False, action='store_true'
    )
    parser.add_argument(
        "--train_disjoint", help="include the disjoin val data in training data (for final model only)", default=False, action='store_true'
    )

    # Feats details
    parser.add_argument(
        "--feats_model", help="feats extractor model" , default=None,
    )
    parser.add_argument(
        "--pretrained_feats_model", help="the name of the pretrained model to use - can be a local save" , default=None,
    )
    parser.add_argument(
        "--layer", help="layer of feat extractor to use" , default=None, type = int
    )
    parser.add_argument(
        "--num_layers", help="layer of feat extractor to use" , default=12, type = int
    )
    parser.add_argument(
        "--whisper_model", help="location of configuation json file", default = "openai/whisper-small"
    )
    parser.add_argument(
        "--whisper_language", help="location of configuation json file", default = "English"
    )
    parser.add_argument(
        "--whisper_task", help="location of configuation json file", default = "transcribe"
    )

    # Classifier model 
    parser.add_argument(
        "--model", help="model type" , default=None,
    )
    parser.add_argument(
        "--restart_model", help="path to a previous training checkpoint" , default=None,
    )
    parser.add_argument(
        "--restart_epoch", help="epoch number to restart from (for logging)" , default=0, type = int
    )

    # Training hyperparameters
    parser.add_argument(
        "--batch_size", help="batch size" , default=None, type=int
    )
    parser.add_argument(
        "--n_epochs", help="number of epochs", default=None, type=int
    )
    parser.add_argument(
        "--lr", help="learning rate", default=None, type=float
    )
    parser.add_argument(
        "--weight_decay", help="weight decay", default=None, type=float
    )
    parser.add_argument(
        "--grad_clipping", help="gradient clipping", default=1, type=float
    )

    # Exemplar bits
    parser.add_argument(
        "--ex_size", help="train split" , default=None, type=int
    )
    parser.add_argument(
        "--p_factor", help="exemplar model p_factor" , default=None, type=int
    )
    parser.add_argument(
        "--minerva_dim", help="exemplar model feat transformation dimnesion" , default=None, type=int
    )
    parser.add_argument(
        "--minerva_r_dim", help="exemplar model feat transformation dimnesion" , default=None, type=int
    )
    parser.add_argument(
        "--num_minervas", help="number of minerva models to use for system combinations or the std model" , default=None, type=int
    )
    parser.add_argument(
        "--random_exemplars", help="use randomly selected exemplars, rather than stratified exemplars", default=False, action='store_true'
    )
    parser.add_argument(
        "--exemplar_source", help="one of CEC1, CEC2, all or matched", default="CEC2"
    )
    parser.add_argument(
        "--use_r_encoding", help="use positional encoding for exemplar correctness", default=False, action='store_true'
    )

    args = parser.parse_args()

    if args.config_file is not None:
        with open(args.config_file) as f:
            config = AttrDict(json.load(f))
    else:
        config = AttrDict()

    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    config["device"] = args.device

    if args.exp_id is None:
        args.exp_id = config["exp_id"] if "exp_id" in config else "test"
    config["exp_id"] = args.exp_id

    if args.wandb_project is None:
        args.wandb_project = config["wandb_project"] if "wandb_project" in config else None
    config["wandb_project"] = args.wandb_project
    
    if args.seed is None:
        args.seed = config["seed"] if "seed" in config else 1234
    config["seed"] = args.seed

    config["debug"] = args.debug
    config["skip_train"] = args.skip_train
    config["use_CPC1"] = args.use_CPC1
    config["N"] = args.N
    config["skip_test"] = args.skip_test

    if args.feats_model is None:
        args.feats_model = config["feats_model"] if "feats_model" in config else "WhisperFull"
    config["feats_model"] = args.feats_model

    if args.pretrained_feats_model is None:
        args.pretrained_feats_model = config["pretrained_feats_model"] if "pretrained_feats_model" in config else None
    config["pretrained_feats_model"] = args.pretrained_feats_model

    args.pretrained_model_dir = f"save/{args.pretrained_model_dir}"
    config["pretrained_model_dir"] = args.pretrained_model_dir

    if args.layer == None:
        args.layer = config["layer"] if "layer" in config else None
    config["layer"] = args.layer

    config["whisper_model"] = args.whisper_model
    config["whisper_language"] = args.whisper_language
    config["whisper_task"] = args.whisper_task

    if args.model is None:
        args.model = config["model"] if "model" in config else None
    config["model"] = args.model

    config["restart_model"] = args.restart_model
    config["restart_epoch"] = args.restart_epoch

    if args.batch_size is None:
        args.batch_size = config["batch_size"] if "batch_size" in config else 8
    config["batch_size"] = args.batch_size

    if args.lr is None:
        args.lr = config["lr"] if "lr" in config else 0.001
    config["lr"] = args.lr

    if args.weight_decay is None:
        args.weight_decay = config["weight_decay"] if "weight_decay" in config else 0.001
    config["weight_decay"] = args.weight_decay

    if args.n_epochs is None:
        args.n_epochs = config["n_epochs"] if "n_epochs" in config else 25
    config["n_epochs"] = args.n_epochs
    
    if args.ex_size is None:
        args.ex_size = config["ex_size"] if "ex_size" in config else None
    config["ex_size"] = args.ex_size

    if args.p_factor is None:
        args.p_factor = config["p_factor"] if "p_factor" in config else 1
    config["p_factor"] = args.p_factor

    if args.minerva_dim is None:
        args.minerva_dim = config["minerva_dim"] if "minerva_dim" in config else None
    config["minerva_dim"] = args.minerva_dim

    if args.minerva_r_dim is None:
        args.minerva_r_dim = config["minerva_r_dim"] if "minerva_r_dim" in config else None
    config["minerva_r_dim"] = args.minerva_r_dim
    
    if args.num_minervas is None:
        args.num_minervas = config["num_minervas"] if "num_minervas" in config else 1
    config["num_minervas"] = args.num_minervas
    
    config["random_exemplars"] = args.random_exemplars
    config["exemplar_source"] = args.exemplar_source
    config["learn_incorrect"] = args.learn_incorrect
    config["use_r_encoding"] = args.use_r_encoding
    config["train_disjoint"] = args.train_disjoint

    if args.use_CPC1:
        args.wandb_project = "CPC1" if args.wandb_project is None else args.wandb_project
        config["wandb_project"] = "CPC1"
        args.dataroot = DATAROOT_CPC1
        args.in_json_file = DATAROOT_CPC1 + "metadata/CPC1.train_indep.json"
        config["in_json_file"] = args.in_json_file
        args.test_json_file = DATAROOT_CPC1 + "metadata/CPC1.test_indep.json"
        config["test_json_file"] = args.test_json_file
    else:
        args.dataroot = DATAROOT
        args.wandb_project = "CPC" if args.wandb_project is None else args.wandb_project
        config["wandb_project"] = args.wandb_project
        args.in_json_file = f"{DATAROOT}/metadata/CEC1.train.{args.N}.json"
        config["in_json_file"] = args.in_json_file
        if args.skip_test:
            args.test_json_file = None
            config["test_json_file"] = None
        else:
            args.test_json_file = f"{DATAROOT}/metadata/CEC2.test.{args.N}.json"
            config["test_json_file"] = args.test_json_file
        
    if args.summ_file is None:
        if args.use_CPC1:
            args.summ_file = "save/CPC1_metrics.csv"
        else:
            args.summ_file = "save/CPC_metrics.csv"
    config["summ_file"] = args.summ_file

    today = datetime.datetime.today()
    date = today.strftime("%H-%M-%d-%b-%Y")
    nm = "" if args.num_minervas == 1 else f"nm{args.num_minervas}"
    args.model_name = "%s_%s_%s_%s_%s_%s_%s"%(args.exp_id,args.N,args.feats_model,args.model,nm,date,args.seed)
    args.model_dir = "save/%s"%(args.model_name)
    config["model_name"] = args.model_name

    if args.out_csv_file is None:
        nm = "" if args.num_minervas == 1 else f"_nm{args.num_minervas}_{args.seed}"
        args.out_csv_file = f"{args.model_dir}/{args.exp_id}_N{args.N}_{args.feats_model}_{args.model}{nm}"
    config["out_csv_file"] = args.out_csv_file
    
    main(args, config)
